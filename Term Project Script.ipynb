{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65db40d-36cf-430e-8f46-b020b24c62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import tqdm\n",
    "\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.log_console = True\n",
    "\n",
    "# -----------------------\n",
    "# USER PARAMETERS\n",
    "# -----------------------\n",
    "demands_csv = \"demands.csv\"       # must have columns: demand_id, lat, lon\n",
    "candidates_csv = \"candidates.csv\" # must have columns: candidate_id, lat, lon\n",
    "od_out_csv = \"od.csv\"\n",
    "network_mode = \"drive\"           # \"drive\",\"drive_service\",\"walk\",\"bike\", etc.\n",
    "crs = \"epsg:4326\"                # WGS84 coords in input files\n",
    "default_speed_kph = 50.0         # fallback speed for edges missing maxspeed\n",
    "use_osm_speeds = True            # use osmnx.add_edge_speeds if True\n",
    "# -----------------------\n",
    "\n",
    "# Load points\n",
    "dem = pd.read_csv(demands_csv, dtype={\"demand_id\": str})\n",
    "cand = pd.read_csv(candidates_csv, dtype={\"candidate_id\": str})\n",
    "\n",
    "# Basic checks\n",
    "for df, name in [(dem, \"demands\"), (cand, \"candidates\")]:\n",
    "    if not {\"lat\", \"lon\", df.columns[0]}.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"{name}.csv must contain columns: id column, lat, lon. \"\n",
    "                         \"Make sure the first column is the ID and named appropriately.\")\n",
    "\n",
    "# Determine bounding box (with small buffer)\n",
    "all_lats = np.concatenate([dem[\"lat\"].values, cand[\"lat\"].values])\n",
    "all_lons = np.concatenate([dem[\"lon\"].values, cand[\"lon\"].values])\n",
    "lat_pad = 0.05  # ~5 km pad (tune for your region)\n",
    "lon_pad = 0.05\n",
    "\n",
    "north = all_lats.max() + lat_pad\n",
    "south = all_lats.min() - lat_pad\n",
    "east  = all_lons.max() + lon_pad\n",
    "west  = all_lons.min() - lon_pad\n",
    "\n",
    "print(\"Downloading/Loading OSM graph for bbox:\", (north, south, east, west))\n",
    "G = ox.graph_from_bbox(north, south, east, west, network_type=network_mode)\n",
    "\n",
    "# Preprocess edges: add speeds and travel time (meters, seconds, minutes)\n",
    "if use_osm_speeds:\n",
    "    # add_edge_speeds will attempt to parse maxspeed tags, otherwise fallback to default values per highway type\n",
    "    try:\n",
    "        G = ox.add_edge_speeds(G)  # kph stored in 'speed_kph' attribute\n",
    "    except Exception as e:\n",
    "        print(\"add_edge_speeds() failed, continuing with defaults. Error:\", e)\n",
    "        # continue; we'll set default speed below\n",
    "# Ensure edge length exists (should)\n",
    "G = ox.add_edge_lengths(G)\n",
    "\n",
    "# Add travel_time (seconds) attribute: if speed known use it, otherwise default\n",
    "for u, v, k, data in G.edges(keys=True, data=True):\n",
    "    length_m = data.get(\"length\", None)  # meters\n",
    "    if length_m is None:\n",
    "        # safety fallback\n",
    "        length_m = 0.0\n",
    "    speed_kph = data.get(\"speed_kph\", None)\n",
    "    if speed_kph is None:\n",
    "        # fallback heuristics: try to read 'maxspeed' or set default\n",
    "        maxspeed = data.get(\"maxspeed\", None)\n",
    "        if isinstance(maxspeed, list):\n",
    "            maxspeed = maxspeed[0] if maxspeed else None\n",
    "        if maxspeed is not None:\n",
    "            try:\n",
    "                # maxspeed could be \"30 mph\" or \"50\"\n",
    "                if isinstance(maxspeed, str) and \"mph\" in maxspeed.lower():\n",
    "                    # parse mph\n",
    "                    val = float(maxspeed.lower().replace(\"mph\",\"\").strip())\n",
    "                    speed_kph = val * 1.60934\n",
    "                else:\n",
    "                    speed_kph = float(maxspeed)\n",
    "            except Exception:\n",
    "                speed_kph = default_speed_kph\n",
    "        else:\n",
    "            speed_kph = default_speed_kph\n",
    "    # compute travel time seconds; length_m / (speed_m_per_s)\n",
    "    speed_m_per_s = speed_kph * 1000.0 / 3600.0\n",
    "    if speed_m_per_s <= 0:\n",
    "        travel_time_s = math.inf\n",
    "    else:\n",
    "        travel_time_s = length_m / speed_m_per_s\n",
    "    data[\"travel_time\"] = travel_time_s\n",
    "    data[\"travel_time_min\"] = travel_time_s / 60.0\n",
    "\n",
    "# Map points to nearest graph nodes\n",
    "print(\"Mapping demand and candidate points to nearest graph nodes...\")\n",
    "# convert to lists of tuples (lon, lat) for osmnx nearest_nodes\n",
    "dem_nodes = ox.nearest_nodes(G, dem[\"lon\"].tolist(), dem[\"lat\"].tolist())\n",
    "cand_nodes = ox.nearest_nodes(G, cand[\"lon\"].tolist(), cand[\"lat\"].tolist())\n",
    "\n",
    "dem[\"nearest_node\"] = dem_nodes\n",
    "cand[\"nearest_node\"] = cand_nodes\n",
    "\n",
    "# Some candidate nodes may be duplicates; we only need unique candidate nodes for multi-source lookups\n",
    "unique_cand_nodes = np.unique(cand_nodes)\n",
    "# Build mapping candidate_node -> list of candidate_ids (since multiple candidates could map to same node)\n",
    "cand_node_to_ids = {}\n",
    "for idx, row in cand.iterrows():\n",
    "    node = row[\"nearest_node\"]\n",
    "    cand_node_to_ids.setdefault(node, []).append(row[\"candidate_id\"])\n",
    "\n",
    "# Prepare for shortest-path computations:\n",
    "# We'll compute single-source shortest path lengths (by travel_time) from each demand node to all nodes,\n",
    "# then extract travel_time to candidate nodes.\n",
    "print(\"Computing shortest-path travel times from each unique demand node to candidate nodes...\")\n",
    "\n",
    "# For performance: if many demands share same nearest_node, compute unique demand nodes only once\n",
    "unique_dem_nodes, inverse_idxs = np.unique(dem_nodes, return_inverse=True)\n",
    "# Map node -> list of demand ids that map to it\n",
    "dem_node_to_ids = {}\n",
    "for i, row in dem.iterrows():\n",
    "    node = row[\"nearest_node\"]\n",
    "    dem_node_to_ids.setdefault(node, []).append(row[\"demand_id\"])\n",
    "\n",
    "# We'll iterate unique_dem_nodes and compute single-source Dijkstra (weight='travel_time')\n",
    "rows = []\n",
    "pbar = tqdm(unique_dem_nodes, desc=\"dem_nodes\")\n",
    "for source_node in pbar:\n",
    "    # compute travel time (seconds) to all graph nodes using Dijkstra\n",
    "    # note: networkx uses edge weights defined in data; use weight='travel_time'\n",
    "    # use nx.single_source_dijkstra_path_length which returns length in same units as weight (seconds)\n",
    "    lengths = nx.single_source_dijkstra_path_length(G, source_node, weight=\"travel_time\")\n",
    "    # For each candidate node, extract length (seconds). If no path, treat as inf\n",
    "    for cand_node in unique_cand_nodes:\n",
    "        t_s = lengths.get(cand_node, math.inf)\n",
    "        # For each candidate_id mapped to cand_node, emit a row\n",
    "        for candidate_id in cand_node_to_ids[cand_node]:\n",
    "            # For each demand id that maps to source_node, emit a row\n",
    "            for demand_id in dem_node_to_ids[source_node]:\n",
    "                # convert seconds -> minutes; if inf, set to large number (or np.nan)\n",
    "                if math.isinf(t_s):\n",
    "                    t_min = np.nan\n",
    "                else:\n",
    "                    t_min = t_s / 60.0\n",
    "                rows.append((demand_id, candidate_id, t_min))\n",
    "\n",
    "# Build DataFrame and write CSV\n",
    "od_df = pd.DataFrame(rows, columns=[\"demand_id\", \"candidate_id\", \"travel_time\"])\n",
    "print(f\"Computed {len(od_df)} OD pairs. Saving to {od_out_csv} ...\")\n",
    "od_df.to_csv(od_out_csv, index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77ee18-d4c6-4cc6-820e-88126d3edd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Maximal Coverage Location Problem using GurobiPy\n",
    "# Compatible with OSMnx-generated od.csv (travel_time in minutes)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from gurobipy import Model, GRB, quicksum\n",
    "\n",
    "# --------------------------\n",
    "# USER PARAMETERS\n",
    "# --------------------------\n",
    "demands_file = \"demands.csv\"        # columns: demand_id, pop, lat, lon\n",
    "candidates_file = \"candidates.csv\"  # columns: candidate_id, lat, lon\n",
    "od_file = \"od.csv\"                  # columns: demand_id, candidate_id, travel_time\n",
    "T = 30                              # coverage threshold (minutes)\n",
    "K = 2                               # max number of facilities to open\n",
    "OUTPUT_SELECTED = \"selected_sites.csv\"\n",
    "OUTPUT_COVERAGE = \"demand_coverage.csv\"\n",
    "# --------------------------\n",
    "\n",
    "# 1. Load data\n",
    "dem = pd.read_csv(demands_file)\n",
    "cand = pd.read_csv(candidates_file)\n",
    "od = pd.read_csv(od_file)\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"demand_id\", \"pop\"]:\n",
    "    if col not in dem.columns:\n",
    "        raise ValueError(f\"demands.csv must contain column: {col}\")\n",
    "if \"candidate_id\" not in cand.columns:\n",
    "    raise ValueError(\"candidates.csv must contain column: candidate_id\")\n",
    "if not all(c in od.columns for c in [\"demand_id\", \"candidate_id\", \"travel_time\"]):\n",
    "    raise ValueError(\"od.csv must contain columns: demand_id, candidate_id, travel_time\")\n",
    "\n",
    "# 2. Sets and parameters\n",
    "I = dem[\"demand_id\"].unique().tolist()\n",
    "J = cand[\"candidate_id\"].unique().tolist()\n",
    "pop = dem.set_index(\"demand_id\")[\"pop\"].to_dict()\n",
    "\n",
    "# 3. Build coverage sets N_i (candidates that cover demand i)\n",
    "print(\"Building coverage sets (within threshold)...\")\n",
    "N = {i: set(od.loc[(od[\"demand_id\"] == i) & (od[\"travel_time\"] <= T), \"candidate_id\"].unique()) for i in I}\n",
    "\n",
    "# Warn if any demand is uncovered\n",
    "uncovered = [i for i, candidates in N.items() if len(candidates) == 0]\n",
    "if uncovered:\n",
    "    print(f\"⚠️ Warning: {len(uncovered)} demands have no candidate within {T} minutes.\")\n",
    "\n",
    "# 4. Build MILP\n",
    "m = Model(\"MaxCoverage\")\n",
    "m.Params.OutputFlag = 1            # print solver log\n",
    "m.Params.MIPGap = 1e-4             # relative optimality gap\n",
    "m.Params.TimeLimit = 600           # seconds\n",
    "\n",
    "# Decision variables\n",
    "x = m.addVars(J, vtype=GRB.BINARY, name=\"x\")  # 1 if facility j opened\n",
    "z = m.addVars(I, vtype=GRB.BINARY, name=\"z\")  # 1 if demand i covered\n",
    "\n",
    "# Coverage constraints\n",
    "for i in I:\n",
    "    if len(N[i]) == 0:\n",
    "        m.addConstr(z[i] == 0, name=f\"cover_{i}_none\")\n",
    "    else:\n",
    "        m.addConstr(z[i] <= quicksum(x[j] for j in N[i]), name=f\"cover_{i}\")\n",
    "\n",
    "# Facility budget constraint\n",
    "m.addConstr(quicksum(x[j] for j in J) <= K, name=\"facility_budget\")\n",
    "\n",
    "# Objective: maximize total population covered\n",
    "m.setObjective(quicksum(pop[i] * z[i] for i in I), GRB.MAXIMIZE)\n",
    "\n",
    "# 5. Solve model\n",
    "m.optimize()\n",
    "\n",
    "# 6. Extract and print results\n",
    "if m.status in [GRB.OPTIMAL, GRB.TIME_LIMIT]:\n",
    "    selected = [j for j in J if x[j].X >= 0.5]\n",
    "    print(f\"\\nSelected facilities ({len(selected)}): {selected}\")\n",
    "\n",
    "    z_opt = {i: int(round(z[i].X)) for i in I}\n",
    "    covered_df = pd.DataFrame({\n",
    "        \"demand_id\": list(z_opt.keys()),\n",
    "        \"covered\": list(z_opt.values()),\n",
    "        \"pop\": [pop[i] for i in I]\n",
    "    })\n",
    "    total_pop = sum(covered_df[\"pop\"])\n",
    "    covered_pop = (covered_df[\"pop\"] * covered_df[\"covered\"]).sum()\n",
    "    coverage_pct = 100 * covered_pop / total_pop\n",
    "    print(f\"\\nTotal population: {total_pop:,.0f}\")\n",
    "    print(f\"Covered population: {covered_pop:,.0f} ({coverage_pct:.2f}% coverage)\")\n",
    "\n",
    "    # 7. Save outputs\n",
    "    pd.DataFrame({\"candidate_id\": selected}).to_csv(OUTPUT_SELECTED, index=False)\n",
    "    covered_df.to_csv(OUTPUT_COVERAGE, index=False)\n",
    "    print(f\"\\nResults written to {OUTPUT_SELECTED} and {OUTPUT_COVERAGE}\")\n",
    "else:\n",
    "    print(\"❌ Optimization did not converge properly. Status:\", m.status)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
